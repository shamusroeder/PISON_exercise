{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PISON_ml_3.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_EoxXf5Rf0ek",
        "colab_type": "text"
      },
      "source": [
        "# Intialization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E_KtcMlB1Wfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Import libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import csv\n",
        "import random\n",
        "from google.colab import files \n",
        "from google.colab import drive\n",
        "from zipfile import ZipFile\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import glob\n",
        "import re\n",
        "import math\n",
        "from tabulate import tabulate\n",
        "import time\n",
        "from scipy import signal\n",
        "import itertools\n",
        "import statistics\n",
        "import warnings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oETRQeFe9NwZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive/') \n",
        "\n",
        "TARGET_FOLDER_NAME = \"Thumb Swipe Data Collection\"\n",
        "PARENT_FOLDER_LOCATION = \"/content/\" + TARGET_FOLDER_NAME\n",
        "\n",
        "MODEL_TO_TRAIN = 'periodogram' # 'periodogram', 'spectrogram', 'simple'\n",
        "\n",
        "NUMBER_OF_EPOCHS = 25\n",
        "BATCH_SIZE = 16\n",
        "WINDOW_LENGTH = 64 #128\n",
        "\n",
        "TIME_RUN = True\n",
        "SHUFFLE_WITHIN_GROUPS = True\n",
        "FORCE_BINARY = False\n",
        "\n",
        "if (MODEL_TO_TRAIN == 'periodogram') or (MODEL_TO_TRAIN == 'spectrogram'):\n",
        "    DATA_CHANNELS_TO_USE = 'channels' #'channels', 'all'\n",
        "else:\n",
        "    DATA_CHANNELS_TO_USE = 'all'\n",
        "\n",
        "if FORCE_BINARY:\n",
        "    NUMBER_OF_UNIQUE_GESTURES = 2\n",
        "    UNIQUE_GESTURE_NAMES = ['other', 'thumb swipe']\n",
        "else:\n",
        "    NUMBER_OF_UNIQUE_GESTURES = 4\n",
        "    UNIQUE_GESTURE_NAMES = ['none', 'thumb out', 'index up', 'thumb swipe']\n",
        "    \n",
        "FOLD_SPLIT_METHOD = 'one rep out'\n",
        "PRESUMED_SAMPLING_FREQUENCY = 600\n",
        "DATA_COLUMN_NAMES = ['timestamp', 'c0', 'c1', 'x_quat', 'y_quat', 'z_quat', 'w_quat', 'label', 'rep_number']\n",
        "\n",
        "RANDOM_SEED_INITIALIZATION = 777\n",
        "\n",
        "random.seed(RANDOM_SEED_INITIALIZATION)\n",
        "np.random.seed(RANDOM_SEED_INITIALIZATION)\n",
        "\n",
        "try: data_unzipped\n",
        "except NameError: \n",
        "    !unzip -uq \"/content/gdrive/My Drive/Thumb Swipe Data Collection.zip\" -d \"/content\"\n",
        "    data_unzipped = None"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "awYk9aJJf7dF",
        "colab_type": "text"
      },
      "source": [
        "# Main Definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uWU0s3zZ58YI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def main(model_choice = None, straight_parameters = None, DATA_CHANNELS_TO_USE = DATA_CHANNELS_TO_USE):\n",
        "    if TIME_RUN:\n",
        "        total_run_start = time.time()\n",
        "    \n",
        "    df = form_total_df()\n",
        "    idx_array, fold_array, unique_subject_list, unique_repetition_list, unique_gesture_list = extract_task_indices(df)\n",
        "    \n",
        "    window_length = WINDOW_LENGTH\n",
        "    window_overlap = .5\n",
        "    \n",
        "    if DATA_CHANNELS_TO_USE == 'channels':\n",
        "        data_channels = ['c0', 'c1']\n",
        "    elif DATA_CHANNELS_TO_USE == 'quat':\n",
        "        data_channels = ['x_quat', 'y_quat', 'z_quat', 'w_quat']\n",
        "    elif DATA_CHANNELS_TO_USE == 'all':\n",
        "        data_channels = ['c0', 'c1', 'x_quat', 'y_quat', 'z_quat', 'w_quat']\n",
        "    else:\n",
        "         raise Exception(\"improper DATA_CHANNELS_TO_USE entered\")   \n",
        "    \n",
        "    input_dim = (window_length, len(data_channels))\n",
        "    output_dim = NUMBER_OF_UNIQUE_GESTURES\n",
        "    \n",
        "    if FORCE_BINARY:\n",
        "        planned_loss_function = \"categorical_hinge\"\n",
        "    else:\n",
        "        planned_loss_function = 'categorical_crossentropy'\n",
        "    \n",
        "    if FOLD_SPLIT_METHOD == 'one fold out':\n",
        "        test_fold_idx = random.choice(range(0, len(fold_array)))\n",
        "        test_fold_target = fold_array[test_fold_idx]\n",
        "    elif FOLD_SPLIT_METHOD == 'one rep out':\n",
        "        test_fold_target = random.choice(range(0, len(unique_repetition_list)))\n",
        "    elif FOLD_SPLIT_METHOD == 'one user out':\n",
        "        test_fold_target = random.choice(range(0, len(unique_subject_list)))\n",
        "    else: \n",
        "        raise Exception(\"FOLD_SPLIT_METHOD chosen is incompatible with this setup\")\n",
        "    \n",
        "    test_data_tensor = np.empty((0, window_length, len(data_channels)))\n",
        "    test_info_array = np.empty((0, 3))\n",
        "    test_fold, remaining_fold = split_fold(fold_array, test_fold_target, split_method = FOLD_SPLIT_METHOD)\n",
        "        \n",
        "    for fold in test_fold:\n",
        "        for idx_pair in fold_to_idx(fold, idx_array):\n",
        "            interim_data_tensor, interim_info = window_data(idx_to_df(idx_pair, df), data_channels = data_channels,\n",
        "                                                            window_length = window_length, \n",
        "                                                            window_overlap = window_overlap)\n",
        "            \n",
        "            test_data_tensor = np.append(test_data_tensor, interim_data_tensor, axis = 0)\n",
        "            test_info_array = np.append(test_info_array, interim_info, axis = 0) \n",
        "    test_labels = test_info_array[:, 0]\n",
        "    test_tuple = (test_data_tensor, encode_labels(test_labels))\n",
        "\n",
        "    def cross_test(parameter_loading = None, model_choice = None):\n",
        "\n",
        "        if FOLD_SPLIT_METHOD == 'one fold out':\n",
        "            cross_fold_iterations = remaining_fold\n",
        "        elif FOLD_SPLIT_METHOD == 'one rep out':\n",
        "            cross_fold_iterations = list(range(0, len(unique_repetition_list)))\n",
        "            cross_fold_iterations.remove(test_fold_target)\n",
        "        elif FOLD_SPLIT_METHOD == 'one user out':\n",
        "            cross_fold_iterations = list(range(0, len(unique_subject_list)))\n",
        "            cross_fold_iterations.remove(test_fold_target)\n",
        "\n",
        "        cross_fold_loss = []  \n",
        "\n",
        "        for leave_out_fold in cross_fold_iterations:\n",
        "            if TIME_RUN:\n",
        "                leave_out_fold_start = time.time()\n",
        "\n",
        "            train_data_tensor = validation_data_tensor = np.empty((0, window_length, len(data_channels)))\n",
        "            train_info_array = validation_info_array = np.empty((0, 3))\n",
        "            validation_fold, train_fold = split_fold(remaining_fold, leave_out_fold, split_method = FOLD_SPLIT_METHOD)\n",
        "\n",
        "            for fold in train_fold:\n",
        "                for idx_pair in fold_to_idx(fold, idx_array):\n",
        "                    interim_data_tensor, interim_info = window_data(idx_to_df(idx_pair, df), \n",
        "                                                                    data_channels = data_channels, \n",
        "                                                                    window_length = window_length,\n",
        "                                                                    window_overlap = window_overlap)\n",
        "                    train_data_tensor = np.append(train_data_tensor, interim_data_tensor, axis = 0)\n",
        "                    train_info_array = np.append(train_info_array, interim_info, axis = 0)\n",
        "\n",
        "            train_labels = train_info_array[:, 0]\n",
        "\n",
        "            if SHUFFLE_WITHIN_GROUPS:\n",
        "                train_data_tensor_ordered = train_data_tensor\n",
        "                train_labels_ordered = train_labels\n",
        "                train_data_tensor, train_labels = concurrent_shuffle(train_data_tensor, train_labels)\n",
        "\n",
        "            train_tuple = (train_data_tensor, encode_labels(train_labels))\n",
        "\n",
        "            for fold in validation_fold:\n",
        "                for idx_pair in fold_to_idx(fold, idx_array):\n",
        "                    interim_data_tensor, interim_info = window_data(idx_to_df(idx_pair, df), \n",
        "                                                                    data_channels = data_channels,\n",
        "                                                                    window_length = window_length,\n",
        "                                                                    window_overlap = window_overlap)\n",
        "                    validation_data_tensor = np.append(validation_data_tensor, interim_data_tensor, axis = 0)\n",
        "                    validation_info_array = np.append(validation_info_array, interim_info, axis = 0) \n",
        "            validation_labels = validation_info_array[:, 0]\n",
        "\n",
        "            if SHUFFLE_WITHIN_GROUPS:\n",
        "                validation_data_tensor_ordered = validation_data_tensor\n",
        "                validation_labels_ordered = validation_labels\n",
        "                validation_data_tensor, validation_labels = concurrent_shuffle(validation_data_tensor, \n",
        "                                                                               validation_labels)\n",
        "\n",
        "            validation_tuple = (validation_data_tensor, encode_labels(validation_labels))\n",
        "\n",
        "            if parameter_loading is None:\n",
        "                if model_choice is None:            \n",
        "                    ml = pure_periodogram_net() \n",
        "                elif model_choice == 'secret model':\n",
        "                    ml = secret_net(input_shape = input_dim)\n",
        "                elif model_choice == 'periodogram':\n",
        "                    ml = pure_periodogram_net()\n",
        "                elif model_choice == 'spectrogram':\n",
        "                    ml = spectrogram_2d_conv_net()\n",
        "                elif model_choice == 'simple':\n",
        "                    ml = simple_net(input_shape = input_dim)\n",
        "                else:\n",
        "                    raise Exception(\"improper model_choice input\")\n",
        "            else:\n",
        "                if model_choice is None:            \n",
        "                    ml = pure_periodogram_net(parameters = parameter_loading) \n",
        "                elif model_choice == 'secret model':\n",
        "                    ml = secret_net(input_shape = input_dim, parameters = parameter_loading)\n",
        "                elif model_choice == 'periodogram':\n",
        "                    ml = pure_periodogram_net(parameters = parameter_loading)\n",
        "                elif model_choice == 'spectrogram':\n",
        "                    ml = spectrogram_2d_conv_net(parameters = parameter_loading)\n",
        "                elif model_choice == 'simple':\n",
        "                    ml = simple_net(input_shape = input_dim, parameters = parameter_loading)\n",
        "                else:\n",
        "                    raise Exception(\"improper model_choice input\")\n",
        "\n",
        "            train_results = ml.train(train_tuple = train_tuple, optimizer = tf.train.AdamOptimizer(),\n",
        "                                     show_progress = 0, verbose = 0, loss = planned_loss_function)\n",
        "            validation_results = ml.evaluate(test_tuple = validation_tuple, verbose = 0)\n",
        "\n",
        "            cross_fold_loss.append(validation_results[0])\n",
        "            \n",
        "        return np.mean(cross_fold_loss)\n",
        "    \n",
        "    if straight_parameters is None:\n",
        "        min_loss = float(\"inf\")\n",
        "        if TIME_RUN:\n",
        "            hyperparameter_tuning_start = time.time()\n",
        "\n",
        "        if model_choice == 'secret model':\n",
        "            parameter_names = ['int_conv_filt', 'ext_conv_filt', 'dense_nodes']\n",
        "            parameter_options = ((8, 16), (4, 8), (16, 32))\n",
        "            assert len(parameter_names) == len(parameter_options)\n",
        "            num_loops = 1\n",
        "            for i, option_name in enumerate(parameter_names):\n",
        "                num_loops = num_loops * len(parameter_options[i])\n",
        "            j = 0\n",
        "            for int_conv_filt in parameter_options[0]:\n",
        "                for ext_conv_filt in parameter_options[1]:\n",
        "                    for dense_nodes in parameter_options[2]:\n",
        "                        interim_parameters = (int_conv_filt, ext_conv_filt, dense_nodes)\n",
        "                        interim_loss = cross_test(parameter_loading = interim_parameters,\n",
        "                                                  model_choice = model_choice)\n",
        "                        j = j + 1\n",
        "                        print(\"Completed loop {} of {}\\nElapsed time = {} minutes\".format(j, num_loops, \n",
        "                                                                                          round((time.time() \n",
        "                                                                                                - total_run_start) \n",
        "                                                                                                / 60, \n",
        "                                                                                                2)))\n",
        "                        print(\"Loss = {}\\nParameters = {}\".format(round(interim_loss, 4), interim_parameters))\n",
        "                        if interim_loss < min_loss:\n",
        "                            min_loss = interim_loss\n",
        "                            best_parameters = interim_parameters\n",
        "                            print(\"\\n\\tNew best model for {}:\\n\\tLoss = {}\\n\\tParameters = {}\\n\".format(model_choice,\n",
        "                                                                                                    round(min_loss, 4),\n",
        "                                                                                                    best_parameters))\n",
        "        \n",
        "        elif model_choice == 'periodogram':\n",
        "            parameter_names = ['num_conv_filt_1', 'num_conv_filt_2', 'dense_nodes']\n",
        "            parameter_options = ((8, 16), (8, 16), (16, 32))\n",
        "            assert len(parameter_names) == len(parameter_options)\n",
        "            num_loops = 1\n",
        "            for i, option_name in enumerate(parameter_names):\n",
        "                num_loops = num_loops * len(parameter_options[i])\n",
        "            j = 0\n",
        "            for int_conv_filt in parameter_options[0]:\n",
        "                for ext_conv_filt in parameter_options[1]:\n",
        "                    for dense_nodes in parameter_options[2]:\n",
        "                        interim_parameters = (int_conv_filt, ext_conv_filt, dense_nodes)\n",
        "                        interim_loss = cross_test(parameter_loading = interim_parameters,\n",
        "                                                  model_choice = model_choice)\n",
        "                        j = j + 1\n",
        "                        print(\"Completed loop {} of {}\\nElapsed time = {} minutes\".format(j, num_loops, \n",
        "                                                                                          round((time.time() \n",
        "                                                                                                - total_run_start) \n",
        "                                                                                                / 60, \n",
        "                                                                                                2)))\n",
        "                        print(\"Loss = {}\\nParameters = {}\".format(round(interim_loss, 4), interim_parameters))\n",
        "                        if interim_loss < min_loss:\n",
        "                            min_loss = interim_loss\n",
        "                            best_parameters = interim_parameters\n",
        "                            print(\"\\n\\tNew best model for {}:\\n\\tLoss = {}\\n\\tParameters = {}\\n\".format(model_choice,\n",
        "                                                                                                    round(min_loss, 4),\n",
        "                                                                                                    best_parameters))\n",
        "        elif model_choice == 'spectrogram':\n",
        "            parameter_names = ['num_conv_filt_1', 'num_conv_filt_2', 'dense_nodes']\n",
        "            parameter_options = ((8, 16), (4, 8), (16, 32))\n",
        "            assert len(parameter_names) == len(parameter_options)\n",
        "            num_loops = 1\n",
        "            for i, option_name in enumerate(parameter_names):\n",
        "                num_loops = num_loops * len(parameter_options[i])\n",
        "            j = 0\n",
        "            for int_conv_filt in parameter_options[0]:\n",
        "                for ext_conv_filt in parameter_options[1]:\n",
        "                    for dense_nodes in parameter_options[2]:\n",
        "                        interim_parameters = (int_conv_filt, ext_conv_filt, dense_nodes)\n",
        "                        interim_loss = cross_test(parameter_loading = interim_parameters,\n",
        "                                                  model_choice = model_choice)\n",
        "                        j = j + 1\n",
        "                        print(\"Completed loop {} of {}\\nElapsed time = {} minutes\".format(j, num_loops, \n",
        "                                                                                          round((time.time() \n",
        "                                                                                                - total_run_start) \n",
        "                                                                                                / 60, \n",
        "                                                                                                2)))\n",
        "                        print(\"Loss = {}\\nParameters = {}\".format(round(interim_loss, 4), interim_parameters))\n",
        "                        if interim_loss < min_loss:\n",
        "                            min_loss = interim_loss\n",
        "                            best_parameters = interim_parameters\n",
        "                            print(\"\\n\\tNew best model for {}:\\n\\tLoss = {}\\n\\tParameters = {}\\n\".format(model_choice,\n",
        "                                                                                                    round(min_loss, 4),\n",
        "                                                                                                    best_parameters))                   \n",
        "        elif model_choice == 'simple':\n",
        "            parameter_names = ['dense_nodes_1', 'dense_nodes_2']\n",
        "            parameter_options = ((32, 64), (32, 64))\n",
        "            assert len(parameter_names) == len(parameter_options)\n",
        "            num_loops = 1\n",
        "            for i, option_name in enumerate(parameter_names):\n",
        "                num_loops = num_loops * len(parameter_options[i])\n",
        "            j = 0\n",
        "            for int_conv_filt in parameter_options[0]:\n",
        "                for ext_conv_filt in parameter_options[1]:\n",
        "                    interim_parameters = (int_conv_filt, ext_conv_filt)\n",
        "                    interim_loss = cross_test(parameter_loading = interim_parameters, model_choice = model_choice)\n",
        "                    j = j + 1\n",
        "                    print(\"Completed loop {} of {}\\nElapsed time = {} minutes\".format(j, num_loops, \n",
        "                                                                                      round((time.time() \n",
        "                                                                                             - total_run_start) \n",
        "                                                                                            / 60, \n",
        "                                                                                            2)))\n",
        "                    print(\"Loss = {}\\nParameters = {}\".format(round(interim_loss, 4), interim_parameters))\n",
        "                    if interim_loss < min_loss:\n",
        "                        min_loss = interim_loss\n",
        "                        best_parameters = interim_parameters\n",
        "                        print(\"\\n\\tNew best model for {}:\\n\\tLoss = {}\\n\\tParameters = {}\\n\".format(model_choice,\n",
        "                                                                                                    round(min_loss, 4),\n",
        "                                                                                                    best_parameters))\n",
        "        \n",
        "        if TIME_RUN:\n",
        "            hyperparameter_tuning_time = round(time.time() - hyperparameter_tuning_start, 3)\n",
        "    \n",
        "    else:\n",
        "        best_parameters = straight_parameters\n",
        "            \n",
        "    train_data_tensor = np.empty((0, window_length, len(data_channels)))\n",
        "    train_info_array = np.empty((0, 3))\n",
        "    for fold in remaining_fold:\n",
        "        for idx_pair in fold_to_idx(fold, idx_array):\n",
        "            interim_data_tensor, interim_info = window_data(idx_to_df(idx_pair, df), \n",
        "                                                                    data_channels = data_channels,\n",
        "                                                                    window_length = window_length, \n",
        "                                                                    window_overlap = window_overlap)\n",
        "            train_data_tensor = np.append(train_data_tensor, interim_data_tensor, axis = 0)\n",
        "            train_info_array = np.append(train_info_array, interim_info, axis = 0)                                      \n",
        "    train_labels = train_info_array[:, 0]\n",
        "        \n",
        "    if SHUFFLE_WITHIN_GROUPS:\n",
        "        train_data_tensor_ordered = train_data_tensor\n",
        "        train_labels_ordered = train_labels\n",
        "        train_data_tensor, train_labels = concurrent_shuffle(train_data_tensor, train_labels)\n",
        "            \n",
        "    train_tuple = (train_data_tensor, encode_labels(train_labels))\n",
        "        \n",
        "    if model_choice == 'secret model':\n",
        "        ml = secret_net(input_shape = input_dim, parameters = best_parameters)\n",
        "    elif model_choice == 'periodogram':\n",
        "        ml = pure_periodogram_net(parameters = best_parameters)\n",
        "    elif model_choice == 'spectrogram':\n",
        "        ml = spectrogram_2d_conv_net(parameters = best_parameters)\n",
        "    elif model_choice == 'simple':\n",
        "        ml = simple_net(input_shape = input_dim, parameters = best_parameters)\n",
        "    else:\n",
        "        raise Exception(\"improper model_choice input\")\n",
        "        \n",
        "    if TIME_RUN:\n",
        "        training_time_start = time.time()\n",
        "            \n",
        "    final_train_results = ml.train(train_tuple = train_tuple, optimizer = tf.train.AdamOptimizer(), \n",
        "                                   loss = planned_loss_function)\n",
        "    final_train_f1 = calculate_f1(train_labels, ml.apply(train_data_tensor, verbose = 0))\n",
        "    \n",
        "    if TIME_RUN:\n",
        "        training_time = round(time.time() - training_time_start, 3)\n",
        "    \n",
        "    test_results =  ml.evaluate(test_tuple = test_tuple, verbose = 0)\n",
        "    predicted_test_labels = ml.apply(test_data_tensor, verbose = 0)\n",
        "    test_f1 = calculate_f1(test_labels, predicted_test_labels)\n",
        "    \n",
        "    if TIME_RUN:\n",
        "        final_run_time = time.time() - total_run_start\n",
        "    \n",
        "    plot_confusion_matrix(test_labels, predicted_test_labels, normalize = False, \n",
        "                          title = \"Confusion Matrix for {} model\".format(ml.model_name))\n",
        "    \n",
        "    print(\"\\nTesting Confusion Matrix:\\n{}\\n\".format(plain_confusion_matrix(test_labels, \n",
        "                                                                            predicted_test_labels, \n",
        "                                                                            normalize = False)))\n",
        "    \n",
        "    if TIME_RUN:\n",
        "        print(\"total runtime = {} minutes\".format(round(final_run_time / 60, 2)))\n",
        "        if straight_parameters is None:\n",
        "            print(\"tuning time = {} minutes\".format(round(hyperparameter_tuning_time / 60, 2)))\n",
        "        print(\"training time = {} minutes\".format(round(training_time / 60 , 2)))\n",
        "    num_loops = 1 \n",
        "    if straight_parameters is None:\n",
        "        for i, option_name in enumerate(parameter_names):\n",
        "            print(\"Possible hyperparameters for {} : {}\\nSelected {}\\n\".format(option_name, \n",
        "                                                                           parameter_options[i], best_parameters[i]))\n",
        "            num_loops = num_loops * len(parameter_options[i])\n",
        "            \n",
        "    print(\"number of epochs = {}\".format(NUMBER_OF_EPOCHS))\n",
        "    print(\"batch size = {}\".format(BATCH_SIZE))\n",
        "    \n",
        "    print(\"total number of hyperparameter combinations tested = {}\".format(num_loops))\n",
        "\n",
        "    print('\\n',tabulate([['data_set','loss', 'accuracy', 'f1'],\n",
        "                         np.concatenate((['training'], np.around(np.array(final_train_results), 3),\n",
        "                                         [round(final_train_f1, 3)])),\n",
        "                         np.concatenate((['testing'], np.around(np.array(test_results), 3),\n",
        "                                         [round(test_f1, 3)]))],\n",
        "                           headers = \"firstrow\"), '\\n')\n",
        "\n",
        "    print(\"Window duration = {} seconds\".format(round(window_length / PRESUMED_SAMPLING_FREQUENCY, 3)))\n",
        "    print(\"Window_length = {}\".format(window_length))\n",
        "    print(\"Window_overlap = {}\".format(window_overlap))\n",
        "    print(\"data_channels used = {}\".format(data_channels))\n",
        "    print(\"Number of model parameters = {}\".format(ml.model.count_params()))  \n",
        "    print(\"Model_name = {}\".format(ml.model_name))\n",
        "    print(\"Model_notes :  {}\".format(ml.notes))\n",
        "    \n",
        "    print(\"best_parameter values : {}\".format(ml.parameters))\n",
        "    \n",
        "    print(\"\\nTraining set statistics:\")\n",
        "    generate_extra_accuracy_metrics(train_labels, ml.apply(train_data_tensor, verbose = 0))\n",
        "    \n",
        "    print(\"\\nTesting set statistics:\")\n",
        "    generate_extra_accuracy_metrics(test_labels, predicted_test_labels)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Xhu4dzbgFIK",
        "colab_type": "text"
      },
      "source": [
        "# ML Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Du2qqygUm-tD",
        "colab_type": "text"
      },
      "source": [
        "## ML flow definition"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pN8uE8gAmPW-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class machine_learning_flow:\n",
        "    def __init__(self, model_assembly, input_shape, output_shape, model_name, parameters = None, \n",
        "                 preprocess = lambda x : x, verbose = False, notes = None):\n",
        "        self.model_name = model_name\n",
        "        self.preprocess = preprocess\n",
        "        self.input_shape = input_shape\n",
        "        self.output_shape = output_shape\n",
        "        self.notes = notes\n",
        "        self.parameters = parameters\n",
        "        \n",
        "        if not callable(input_shape):\n",
        "            self.model = model_assembly(input_shape, output_shape, parameters = parameters)\n",
        "            if verbose:\n",
        "                self.model.summary()\n",
        "            self.model_assembled = True\n",
        "        else:\n",
        "            self.model = model_assembly\n",
        "            self.model_assembled = False\n",
        "        \n",
        "    def apply(self, x, verbose = 1, postprocessing = False):\n",
        "        \n",
        "        def predict_classes(input_model, X, verbose = 1):\n",
        "            proba = input_model.predict(X, batch_size= BATCH_SIZE, verbose = min(1 ,verbose))\n",
        "            return proba.argmax(axis=-1)\n",
        "        \n",
        "        if postprocessing:\n",
        "            return predict_classes(self.model, x, verbose = verbose)\n",
        "        else:\n",
        "            return predict_classes(self.model, self.preprocess(x), verbose = verbose)\n",
        "    \n",
        "    def train(self, train_tuple, validation_tuple = None, verbose = 2, show_progress = True, \n",
        "              optimizer = tf.keras.optimizers.Adam(lr = 0.001), loss = 'categorical_crossentropy', \n",
        "              number_of_epochs = NUMBER_OF_EPOCHS, metrics = ['accuracy']):\n",
        "        \n",
        "        if self.model_assembled: #if input shape is not uniquely dependent on the input itself\n",
        "            if verbose > 0:\n",
        "                self.model.summary()\n",
        "        else:\n",
        "            interim_input_shape = self.input_shape(train_tuple[0])\n",
        "            self.model = self.model(interim_input_shape, self.output_shape, parameters = self.parameters)\n",
        "            if verbose > 0:\n",
        "                self.model.summary()\n",
        "            self.model_assembled = True\n",
        "            \n",
        "        if validation_tuple is not None:\n",
        "            has_validation = True\n",
        "        else:\n",
        "            has_validation = False\n",
        "        \n",
        "        \n",
        "        class_weight = None\n",
        "        \n",
        "        if show_progress:\n",
        "            print(\"Model being trained:\", self.model_name)\n",
        "        \n",
        "        \n",
        "        self.model.compile(optimizer = optimizer, loss = loss, metrics = metrics) \n",
        "\n",
        "        if validation_tuple is not None:\n",
        "            num_validation_steps = math.ceil(validation_tuple[0].shape[0] / BATCH_SIZE)\n",
        "            \n",
        "            history = self.model.fit(self.preprocess(train_tuple[0]), \n",
        "                                     train_tuple[1], \n",
        "                                     epochs = number_of_epochs, \n",
        "                                     batch_size = BATCH_SIZE, \n",
        "                                     verbose = verbose,\n",
        "                                     validation_data = (self.preprocess(validation_tuple[0]), \n",
        "                                                        validation_tuple[1]), \n",
        "                                     validation_steps = num_validation_steps,\n",
        "                                     class_weight = class_weight) \n",
        "            results = (self.model.evaluate(self.preprocess(train_tuple[0]), train_tuple[1], verbose = 0),\n",
        "                       self.model.evaluate(self.preprocess(validation_tuple[0]), validation_tuple[1], verbose = 0))\n",
        "        else:\n",
        "            \n",
        "            history = self.model.fit(self.preprocess(train_tuple[0]),\n",
        "                                     train_tuple[1], \n",
        "                                     epochs = NUMBER_OF_EPOCHS, \n",
        "                                     batch_size = BATCH_SIZE,\n",
        "                                     verbose = verbose,\n",
        "                                     class_weight = class_weight)  \n",
        "            \n",
        "            results = self.model.evaluate(self.preprocess(train_tuple[0]), train_tuple[1], verbose = 0)\n",
        "        if verbose:\n",
        "            loss_plot(history, has_validation = has_validation)\n",
        "        \n",
        "            print('\\nResults of model testing:')\n",
        "        return results\n",
        "    \n",
        "    def evaluate(self, test_tuple, verbose = 1):\n",
        "        return self.model.evaluate(self.preprocess(test_tuple[0]), test_tuple[1], verbose = verbose)\n",
        "    \n",
        "    def force_input_shape(self, new_input_shape):\n",
        "        assert self.model_assembled == False, \"Forcing input shape on an already assembled model\"\n",
        "        self.input_shape = new_input_shape\n",
        "        self.model = model_assembly(self.input_shape, self.output_shape, parameters = self.parameters)\n",
        "        self.model_assembled = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uWoTntVnCrz",
        "colab_type": "text"
      },
      "source": [
        "## 2D_conv spectrogram nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rxaPJY9xo-2P",
        "colab_type": "text"
      },
      "source": [
        "### Basic spectrogram_2d_conv_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NotBiJqZyQqp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def spectrogram_2d_conv_net(input_shape = None, output_shape = NUMBER_OF_UNIQUE_GESTURES, nperseg = 16, \n",
        "                            parameters = (16, 8, 32), notes = ''):\n",
        "    \n",
        "    input_shape_lambda = lambda incoming_data: input_shape_for_conv_spectrogram(incoming_data, nperseg = nperseg)\n",
        "    preprocess_lambda = lambda incoming_data: preprocess_input_for_spectrogram(incoming_data, nperseg = nperseg)\n",
        "    \n",
        "    return machine_learning_flow(spectrogram_2d_conv_net_architecture, \n",
        "                                 input_shape = input_shape_lambda, \n",
        "                                 output_shape = output_shape,\n",
        "                                 model_name = 'spectrogram_2d_conv_net', \n",
        "                                 preprocess = preprocess_lambda,\n",
        "                                 parameters = parameters,\n",
        "                                 notes = [\"nperseg = {}\".format(nperseg)] + [notes])\n",
        "\n",
        "def spectrogram_2d_conv_net_architecture(in_dim, out_dim, parameters):\n",
        "    assert len(parameters) == 3, \"improper number of parameters entered into spectrogram_2d_conv_net_architecture\"\n",
        "    \n",
        "    (first_conv_layer_filters, second_conv_layer_filters, number_of_dense_nodes) = parameters\n",
        "    \n",
        "    model = tf.keras.models.Sequential()\n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape = in_dim, axis = -1))\n",
        "    \n",
        "    model.add(tf.keras.layers.Conv2D(first_conv_layer_filters, kernel_size = 2))\n",
        "    model.add(tf.keras.layers.Conv2D(second_conv_layer_filters, kernel_size = 2))\n",
        "    model.add(tf.keras.layers.MaxPooling2D())\n",
        "    \n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(number_of_dense_nodes, activation='relu'))\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(out_dim, activation = 'softmax'))\n",
        "  \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ru4j7TSIoWxH",
        "colab_type": "text"
      },
      "source": [
        "### Setup functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D8UZ9olGoWUG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def input_shape_for_conv_spectrogram(x, nperseg = 16):\n",
        "    return preprocess_input_for_spectrogram(x[0:1], nperseg = nperseg).shape[1:]\n",
        " \n",
        "def preprocess_input_for_spectrogram(x, nperseg = 16):\n",
        "    def spectrogram_transform(y):\n",
        "        y = signal.spectrogram(y, fs = PRESUMED_SAMPLING_FREQUENCY, nperseg = nperseg, window = 'hamming', axis = 0)[-1]\n",
        "        return np.expand_dims(np.transpose(y, (0,2,1)), axis = 0)\n",
        "    \n",
        "    output_spectrogram = spectrogram_transform(x[0])\n",
        "    \n",
        "    for i in range(1, x.shape[0]):\n",
        "        output_spectrogram = np.append(output_spectrogram, spectrogram_transform(x[i]), axis = 0)\n",
        "    \n",
        "    return output_spectrogram"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TjAkjZ3vW6bj",
        "colab_type": "text"
      },
      "source": [
        "## Periodogram nets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEuyE7G0XEYP",
        "colab_type": "text"
      },
      "source": [
        "### Pure Periodogram net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aLTiCOw-XDpC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def pure_periodogram_net(input_shape = None, output_shape = NUMBER_OF_UNIQUE_GESTURES, \n",
        "                         parameters = (16, 16, 16), notes = ''):\n",
        "    \n",
        "    return machine_learning_flow(pure_periodogram_net_architecture, \n",
        "                                 input_shape = input_shape_for_pure_periodogram, \n",
        "                                 output_shape = output_shape,\n",
        "                                 model_name = 'pure_periodogram_net', \n",
        "                                 preprocess = transform_to_pure_periodogram,\n",
        "                                 parameters = parameters,\n",
        "                                 notes = [notes])\n",
        "\n",
        "def pure_periodogram_net_architecture(in_dim, out_dim, parameters = (16, 16, 16)):\n",
        "    assert len(parameters) == 3, \"improper number of parameters entered into pure_periodogram_net_architecture\"\n",
        "    \n",
        "    channel_setup = 'channels_last'\n",
        "    \n",
        "    number_of_internal_conv_filters = parameters[0]\n",
        "    number_of_external_conv_filters = parameters[1]\n",
        "    number_of_dense_nodes = parameters[2]\n",
        "    \n",
        "    incoming_data = tf.keras.layers.Input(shape = in_dim, name = 'main_input') \n",
        "    \n",
        "    if in_dim[-1] == 2:\n",
        "        has_quat = False\n",
        "    else:\n",
        "        has_quat = True\n",
        "    transformed_data = tf.keras.layers.BatchNormalization(axis = -1, name = 'input_batch_normalization_layer')(incoming_data)\n",
        "    #transformed_data = tf.keras.layers.Lambda(lambda z: transform_to_pure_periodogram(z[:, :, 0:2]))(incoming_data)\n",
        "    \n",
        "    def channel_metrics(x, channel, channel_label = None, channel_setup = \"channels_last\"):\n",
        "        y = tf.keras.layers.Lambda(lambda z: tf.expand_dims(z[:, :, channel], -1),\n",
        "                                   name = 'metric_lambda_extraction_' + channel_label)(x)\n",
        "        y_mean = tf.keras.layers.Lambda(lambda z: tf.math.reduce_mean(z, axis = 1, keepdims = True), \n",
        "                                        name = 'metric_lambda_mean_' + channel_label)(y)\n",
        "        y_std = tf.keras.layers.Lambda(lambda z: tf.math.reduce_std(z, axis = 1, keepdims = True), \n",
        "                                       name = 'metric_lambda_std_' + channel_label)(y)\n",
        "        y_max = tf.keras.layers.Lambda(lambda z: tf.math.reduce_max(z, axis = 1, keepdims = True), \n",
        "                                       name = 'metric_lambda_max_' + channel_label)(y)\n",
        "        y_min = tf.keras.layers.Lambda(lambda z: tf.math.reduce_min(z, axis = 1, keepdims = True), \n",
        "                                       name = 'metric_lambda_min_' + channel_label)(y)\n",
        "        \n",
        "        y = tf.keras.layers.concatenate([y_mean, y_std, y_max, y_min], axis = 1)\n",
        "        \n",
        "        return y\n",
        "    \n",
        "    if has_quat:\n",
        "        x_quat_metric_isolated_model = tf.keras.models.Model(inputs = incoming_data, \n",
        "                                                    outputs = channel_metrics(transformed_data, 2,\n",
        "                                                                              channel_label = 'x_quat'))\n",
        "        y_quat_metric_isolated_model = tf.keras.models.Model(inputs = incoming_data, \n",
        "                                                    outputs = channel_metrics(transformed_data, 3, \n",
        "                                                                              channel_label = 'y_quat'))\n",
        "        z_quat_metric_isolated_model = tf.keras.models.Model(inputs = incoming_data, \n",
        "                                                    outputs = channel_metrics(transformed_data, 4, \n",
        "                                                                              channel_label = 'z_quat'))\n",
        "        w_quat_metric_isolated_model = tf.keras.models.Model(inputs = incoming_data, \n",
        "                                                    outputs = channel_metrics(transformed_data, 5, \n",
        "                                                                              channel_label = 'w_quat'))\n",
        "        \n",
        "        metric_output = tf.keras.layers.concatenate([x_quat_metric_isolated_model.output, \n",
        "                                                     y_quat_metric_isolated_model.output, \n",
        "                                                     z_quat_metric_isolated_model.output,\n",
        "                                                     w_quat_metric_isolated_model.output], \n",
        "                                                    axis = -1)\n",
        "        \n",
        "    \n",
        "    combined_output = tf.keras.layers.Conv1D(number_of_internal_conv_filters, kernel_size = 3, strides = 1, padding = 'same', \n",
        "                                   activation = 'relu', data_format = channel_setup)(transformed_data)\n",
        "    \n",
        "    combined_output = tf.keras.layers.MaxPooling1D(pool_size = 2, strides = None, padding = 'valid', \n",
        "                                           data_format = channel_setup)(combined_output)\n",
        "    \n",
        "    combined_output = tf.keras.layers.Conv1D(number_of_external_conv_filters, kernel_size = 2, strides = 1,\n",
        "                                             activation = 'relu', padding = 'same', \n",
        "                                             data_format = channel_setup,)(combined_output)\n",
        "    \n",
        "    combined_output = tf.keras.layers.Conv1D(number_of_external_conv_filters, kernel_size = 2, strides = 1,\n",
        "                                             activation = 'relu', padding = 'same', \n",
        "                                             data_format = channel_setup,)(combined_output)\n",
        "        \n",
        "    combined_output = tf.keras.layers.MaxPooling1D(pool_size = 2, strides = None, padding = 'valid', \n",
        "                                           data_format = channel_setup)(combined_output)\n",
        "    \n",
        "    combined_output = tf.keras.layers.Flatten()(combined_output)\n",
        "    \n",
        "    if has_quat:\n",
        "        metric_output = tf.keras.layers.BatchNormalization(axis = -1, \n",
        "                                                           name = 'metric_batch_normalization_layer')(metric_output) \n",
        "        metric_output = tf.keras.layers.Flatten()(metric_output)\n",
        "        combined_output = tf.keras.layers.concatenate([combined_output, metric_output], axis = -1)\n",
        "    \n",
        "    combined_output = tf.keras.layers.Dense(number_of_dense_nodes, activation = 'relu')(combined_output)\n",
        "    combined_output = tf.keras.layers.BatchNormalization()(combined_output)\n",
        "    \n",
        "    combined_output = tf.keras.layers.Dense(out_dim, activation='softmax')(combined_output)\n",
        "        \n",
        "    model = tf.keras.models.Model(inputs = incoming_data, outputs = combined_output)\n",
        "  \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izOImkJxX1EQ",
        "colab_type": "text"
      },
      "source": [
        "### Setup Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JYXzUoMQX4aP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def transform_to_pure_periodogram(x, times = None, method = 'normal'):\n",
        "    if method == 'normal':\n",
        "        _, Pxx = signal.periodogram(x, fs = PRESUMED_SAMPLING_FREQUENCY, window = 'hamming', \n",
        "                                    scaling = 'density', axis = 1)\n",
        "        return Pxx\n",
        "    elif method == 'lombscargle':\n",
        "        return None\n",
        "\n",
        "def input_shape_for_pure_periodogram(x, times = None, method = 'normal'):\n",
        "    y = transform_to_pure_periodogram(x[0:2, :, 0:2], times = times, method = method)\n",
        "    return y.shape[1:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R-hf6VwToeTh",
        "colab_type": "text"
      },
      "source": [
        "## Simple nets "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOGLrJ5CfV9s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def simple_net(input_shape, output_shape = NUMBER_OF_UNIQUE_GESTURES, parameters = (32, 32)):\n",
        "    return machine_learning_flow(simple_net_architecture, \n",
        "                                 output_shape = output_shape,\n",
        "                                 input_shape = input_shape, \n",
        "                                 model_name = 'super simple dense net',\n",
        "                                 parameters = parameters)\n",
        "\n",
        "def simple_net_architecture(in_dim, out_dim, parameters = (32, 32)):\n",
        "    assert len(parameters) == 2, \"improper number of parameters entered into simple_net_architecture\"\n",
        "    \n",
        "    (number_of_dense_nodes_1, number_of_dense_nodes_2) = parameters\n",
        "    model = tf.keras.models.Sequential()\n",
        "    \n",
        "    model.add(tf.keras.layers.BatchNormalization(input_shape = in_dim))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(number_of_dense_nodes_1, activation='relu'))\n",
        "    \n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(number_of_dense_nodes_2, activation='relu'))\n",
        "    \n",
        "    \n",
        "    \n",
        "    model.add(tf.keras.layers.Dense(out_dim, activation = 'softmax'))\n",
        "        \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJOMh5JryLEe",
        "colab_type": "text"
      },
      "source": [
        "## Secret Net"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1xH6TEWr587",
        "colab_type": "text"
      },
      "source": [
        "# Other Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6j-v7rzigc0w",
        "colab_type": "text"
      },
      "source": [
        "## Data Loading Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1jc4vuQv3SRP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def form_total_df():\n",
        "    file_path = glob.glob(PARENT_FOLDER_LOCATION + \"/***.csv\")\n",
        "    file_path.sort()\n",
        "\n",
        "    total_df = pd.DataFrame(columns = DATA_COLUMN_NAMES + ['subject_id'])\n",
        "    for file in file_path:\n",
        "        subject_id = int(file.split(\"/S\")[-1].split(\"_\")[0])\n",
        "        holding_df = pd.read_csv(file, header = None)\n",
        "        holding_df.columns = DATA_COLUMN_NAMES\n",
        "        holding_df.sort_values(by = ['timestamp'], inplace = True)\n",
        "        holding_df['subject_id'] = [subject_id] * len(holding_df)\n",
        "        total_df = total_df.append(holding_df, ignore_index = True)\n",
        "    \n",
        "    if FORCE_BINARY:\n",
        "        total_df.replace({'label': [1, 2]}, 0, inplace = True)\n",
        "        total_df.replace({'label': 3}, 1, inplace = True)\n",
        "        \n",
        "    return total_df\n",
        "\n",
        "def form_dict_df():\n",
        "    file_path = glob.glob(PARENT_FOLDER_LOCATION + \"/***.csv\")\n",
        "    file_path.sort()\n",
        "    \n",
        "    data_dict = {}\n",
        "    for file in file_path:\n",
        "        subject_id = int(file.split(\"/S\")[-1].split(\"_\")[0])\n",
        "        data_dict[subject_id] = pd.read_csv(file, header = None)\n",
        "        data_dict[subject_id].columns = DATA_COLUMN_NAMES  \n",
        "        data_dict[subject_id].sort_values(by = ['timestamp'], inplace = True)\n",
        "        \n",
        "    return data_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lImphKq2g2Q_",
        "colab_type": "text"
      },
      "source": [
        "## Data Extraction Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzHphU2OjKf1",
        "colab_type": "code",
        "outputId": "9388b32d-e2e2-4f94-a8c3-4c82cbe423d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "def extract_task_indices(df):\n",
        "    unique_subject_list = df['subject_id'].unique()\n",
        "    unique_gesture_list = df['label'].unique()\n",
        "    unique_repetition_list = df['rep_number'].unique()\n",
        "    \n",
        "    idx_array = np.empty((len(unique_subject_list) \n",
        "                                     * len(unique_repetition_list) \n",
        "                                     * len(unique_gesture_list), \n",
        "                                     2 + 3),\n",
        "                                   dtype = np.uint32)\n",
        "    fold_array = []\n",
        "    \n",
        "    m = 0\n",
        "    for i, subject in enumerate(unique_subject_list):\n",
        "        subject_holding_df =  df.loc[df['subject_id'] == subject]\n",
        "        for j, repetition in enumerate(unique_repetition_list):\n",
        "            repetition_holding_df = subject_holding_df.loc[subject_holding_df['rep_number'] == repetition]\n",
        "            fold_array.append((i,j))     \n",
        "            for k, gesture in enumerate(unique_gesture_list):\n",
        "                transient_idx_array = repetition_holding_df.loc[repetition_holding_df['label'] == gesture].index\n",
        "                \n",
        "                idx_array[m] = np.array([transient_idx_array[0], \n",
        "                                         transient_idx_array[-1] + 1, # add 1 for the purposes of slicing\n",
        "                                         i,j,k])\n",
        "                m = m + 1\n",
        "                \n",
        "    return idx_array, fold_array, unique_subject_list, unique_repetition_list, unique_gesture_list\n",
        "\n",
        "def split_fold(fold_array, extract_fold, split_method = 'one fold out'): \n",
        "    if split_method == 'one fold out':\n",
        "        if (type(extract_fold) is not tuple):\n",
        "            raise Exception\n",
        "        extract_fold = [extract_fold]\n",
        "        pulled_fold = extract_fold\n",
        "        trimmed_fold = [x for x in fold_array if x not in extract_fold]\n",
        "        \n",
        "    elif split_method == 'one user out':    \n",
        "        if (type(extract_fold) is not int):\n",
        "            raise Exception \n",
        "        \n",
        "        extract_fold = [x for x in fold_array if x[0] == extract_fold]\n",
        "        pulled_fold = [x for x in fold_array if x in extract_fold]\n",
        "        trimmed_fold = [x for x in fold_array if x not in extract_fold]\n",
        "        \n",
        "    elif split_method == 'one rep out':    \n",
        "        if (type(extract_fold) is not int):\n",
        "            raise Exception \n",
        "        \n",
        "        extract_fold = [x for x in fold_array if x[1] == extract_fold]\n",
        "        pulled_fold = [x for x in fold_array if x in extract_fold]\n",
        "        trimmed_fold = [x for x in fold_array if x not in extract_fold]\n",
        "        \n",
        "    elif split_method == 'specific':\n",
        "        assert (type(extract_fold) is list) and (type(extract_fold[0]) is tuple)\n",
        "            \n",
        "        pulled_fold = [x for x in fold_array if x in extract_fold]\n",
        "        trimmed_fold = [x for x in fold_array if x not in extract_fold]\n",
        "        \n",
        "    else:\n",
        "        raise Exception(\"improper split_method name\")\n",
        "    \n",
        "    return pulled_fold, trimmed_fold\n",
        "\n",
        "\n",
        "def fold_to_idx(fold, idx_array):\n",
        "    assert type(fold) is tuple\n",
        "    sub_idx = idx_array[np.all(idx_array[:,2:4] == fold, axis = -1)][:, 0:2]\n",
        "    return sub_idx\n",
        "\n",
        "def idx_to_df(idx_pair, df):\n",
        "    sub_df = df.iloc[idx_pair[0]: idx_pair[1]]\n",
        "    return sub_df\n",
        "    \n",
        "def window_data(df, data_channels = ['c0', 'c1', 'x_quat', 'y_quat', 'z_quat', 'w_quat'], window_length = 512, \n",
        "                window_overlap = .5, remainder_cutoff = .25, keep_remainder = True):\n",
        "       \n",
        "    window_shift = int(window_length * (1 - window_overlap))\n",
        "    number_of_perfect_windows = int(np.floor((len(df) - window_length) / window_shift + 1))\n",
        "    remaining_samples = int((len(df) - window_length) % window_shift)\n",
        "    \n",
        "    assert (number_of_perfect_windows > 0, \n",
        "            \"dataframe is {} samples, window is {} samples\".format(len(df), window_length))\n",
        "    \n",
        "    if remaining_samples < remainder_cutoff * window_length:\n",
        "        keep_remainder = False\n",
        "    \n",
        "    relevant_info = np.tile(df[['label', 'rep_number', 'subject_id']].values[0], \n",
        "                            (number_of_perfect_windows + keep_remainder, 1))\n",
        "    \n",
        "    data_tensor = np.empty((number_of_perfect_windows + keep_remainder,\n",
        "                            window_length,\n",
        "                            len(data_channels)),\n",
        "                           dtype = np.float64)\n",
        "    \n",
        "    df_values = df[data_channels].values\n",
        "        \n",
        "    for i in range(0, number_of_perfect_windows):\n",
        "        data_tensor[i] = df_values[i * window_shift: i * window_shift + window_length]\n",
        "        \n",
        "    if keep_remainder:\n",
        "        data_tensor[-1] = df_values[-window_length:]\n",
        "    \n",
        "    return data_tensor, relevant_info\n",
        "    \n",
        "def concurrent_shuffle(data_tensor, labels):\n",
        "    assert (np.shape(labels)[0] == np.shape(data_tensor)[0], \n",
        "            \"received {} labels for {} samples\".format(len(labels), np.shape(data_tensor)[0]))\n",
        "    random_permutation = np.random.permutation(np.shape(labels)[0])\n",
        "    shuffled_labels = labels[random_permutation]\n",
        "    shuffled_data_tensor = data_tensor[random_permutation]\n",
        "    \n",
        "    return shuffled_data_tensor, shuffled_labels\n",
        "\n",
        "\n",
        "def encode_labels(incoming_labels):\n",
        "    return tf.keras.utils.to_categorical(incoming_labels)"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<ipython-input-50-8d09f39616ea>:81: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (number_of_perfect_windows > 0,\n",
            "<ipython-input-50-8d09f39616ea>:106: SyntaxWarning: assertion is always true, perhaps remove parentheses?\n",
            "  assert (np.shape(labels)[0] == np.shape(data_tensor)[0],\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IN4WA2agiaO",
        "colab_type": "text"
      },
      "source": [
        "## Data Analysis Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZGNV4NVDgH5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_sampling_frequency(diff_timestamps, cutoff_diff = 1.9):\n",
        "    mean_delta = np.mean(diff_timestamps)\n",
        "    std_delta = np.std(diff_timestamps)\n",
        "    max_delta = np.max(diff_timestamps)\n",
        "    min_delta = np.min(diff_timestamps)\n",
        "    frac_cutoff = np.sum(diff_timestamps > cutoff_diff)/len(diff_timestamps)\n",
        "    \n",
        "    return mean_delta, std_delta, max_delta, min_delta, frac_cutoff\n",
        "\n",
        "def calculate_f1(true_labels, predicted_labels, num_of_classes = NUMBER_OF_UNIQUE_GESTURES):\n",
        "    f1 = 0.0\n",
        "    for x in range(0,num_of_classes):\n",
        "        class_count = np.sum(true_labels == x)\n",
        "        \n",
        "        if class_count == 0:\n",
        "            continue\n",
        "        precision =  calculate_precision(true_labels, predicted_labels, x)\n",
        "        recall = calculate_recall(true_labels, predicted_labels, x)\n",
        "        f1 = f1 + 2 * class_count / len(true_labels) * precision * recall / (precision + recall)\n",
        "    return f1\n",
        "\n",
        "def calculate_precision(true_labels, predicted_labels, i):\n",
        "    num_true_positives = np.sum((predicted_labels == i) & (true_labels == i))\n",
        "    num_true_negatives = np.sum((predicted_labels != i) & (true_labels != i))\n",
        "    num_false_positives = np.sum((predicted_labels == i) & (true_labels != i))\n",
        "    num_false_negatives = np.sum((predicted_labels != i) & (true_labels == i)) \n",
        "    \n",
        "    precision = num_true_positives / (num_false_positives + num_true_positives)\n",
        "    return precision\n",
        "    \n",
        "def calculate_recall(true_labels, predicted_labels, i):\n",
        "    num_true_positives = np.sum((predicted_labels == i) & (true_labels == i))\n",
        "    num_true_negatives = np.sum((predicted_labels != i) & (true_labels != i))\n",
        "    num_false_positives = np.sum((predicted_labels == i) & (true_labels != i))\n",
        "    num_false_negatives = np.sum((predicted_labels != i) & (true_labels == i))\n",
        "    \n",
        "    recall = num_true_positives / (num_true_positives + num_false_negatives)\n",
        "    \n",
        "    return recall\n",
        "\n",
        "def generate_extra_accuracy_metrics(test_set_labels, predicted_task_labels, task_vector = None):\n",
        "    if task_vector is None:\n",
        "        task_vector = np.sort(np.unique(test_set_labels).astype(dtype=np.int32))\n",
        "    \n",
        "    label_acc = np.zeros(task_vector.shape, dtype=np.float32)\n",
        "    test_set_labels = np.squeeze(test_set_labels)\n",
        "    \n",
        "    sensitivity = np.zeros(task_vector.shape, dtype=np.float32)\n",
        "    precision = np.zeros(task_vector.shape, dtype=np.float32)\n",
        "    balanced_accuracy = np.zeros(task_vector.shape, dtype=np.float32)\n",
        "    f1_score = np.zeros(task_vector.shape, dtype=np.float32)\n",
        "    for i in task_vector: \n",
        "        num_true_positives = np.sum((predicted_task_labels == i) & (test_set_labels == i))\n",
        "        num_true_negatives = np.sum((predicted_task_labels != i) & (test_set_labels != i))\n",
        "        num_false_positives = np.sum((predicted_task_labels == i) & (test_set_labels != i))\n",
        "        num_false_negatives = np.sum((predicted_task_labels != i) & (test_set_labels == i))\n",
        "    \n",
        "        if num_true_positives + num_false_negatives == 0:\n",
        "            sensitivity[i] = 0.\n",
        "            balanced_accuracy[i] = round(num_true_negatives / (num_false_positives + num_true_negatives), 3)\n",
        "        else:\n",
        "            sensitivity[i] = np.round(num_true_positives / (num_true_positives + num_false_negatives), 3)\n",
        "            balanced_accuracy[i] = round((sensitivity[i] \n",
        "                                      + num_true_negatives / (num_false_positives + num_true_negatives)) / 2, 3)\n",
        "    \n",
        "        if num_false_positives + num_true_positives == 0:\n",
        "            precision[i] = 0.\n",
        "        else:\n",
        "            precision[i] = round(num_true_positives / (num_false_positives + num_true_positives), 3)\n",
        "        \n",
        "        f1_score[i] = round((2 * precision[i] * sensitivity[i]) / (precision[i] + sensitivity[i]), 3)\n",
        "        \n",
        "    print(tabulate([np.concatenate((['task_num'], task_vector)), \n",
        "                    np.concatenate((['sensitivity/recall'], sensitivity)),\n",
        "                    np.concatenate((['precision'], precision)),\n",
        "                    np.concatenate((['f1_score'], f1_score)),\n",
        "                    np.concatenate((['balanced_accuracy'], balanced_accuracy))],\n",
        "                   headers = \"firstrow\"), '\\n')  \n",
        "    return None\n",
        "    \n",
        "def loss_plot(history, has_validation = True):\n",
        "    plt.plot(history.history['loss'], label = 'train')\n",
        "    if has_validation:\n",
        "        plt.plot(history.history['val_loss'], label = 'val')\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    return None\n",
        "\n",
        "def plain_confusion_matrix(true_labels,\n",
        "                          predicted_labels,\n",
        "                          normalize = True):\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        cm = np.array(sess.run(tf.confusion_matrix(true_labels.astype(np.int32), \n",
        "                                                   predicted_labels.astype(np.int32))), dtype = np.int32)\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "        \n",
        "    return cm    \n",
        "\n",
        "def plot_confusion_matrix(true_labels,\n",
        "                          predicted_labels,\n",
        "                          target_names = UNIQUE_GESTURE_NAMES,\n",
        "                          title = 'Confusion matrix',\n",
        "                          cmap = None,\n",
        "                          normalize = True):\n",
        "    sess = tf.Session()\n",
        "    with sess.as_default():\n",
        "        cm = np.array(sess.run(tf.confusion_matrix(true_labels.astype(np.int32), \n",
        "                                                   predicted_labels.astype(np.int32))), dtype = np.int32)\n",
        "\n",
        "    accuracy = np.trace(cm) / float(np.sum(cm))\n",
        "    misclass = 1 - accuracy\n",
        "\n",
        "    if cmap is None:\n",
        "        cmap = plt.get_cmap('Blues')\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    \n",
        "    if target_names is not None:\n",
        "        tick_marks = np.arange(len(target_names))\n",
        "        plt.xticks(tick_marks, target_names, rotation=45)\n",
        "        plt.yticks(tick_marks, target_names)\n",
        "\n",
        "    if normalize:\n",
        "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "\n",
        "    thresh = cm.max() / 1.5 if normalize else cm.max() / 2\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        if normalize:\n",
        "            plt.text(j, i, \"{:0.2f}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "        else:\n",
        "            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n",
        "                     horizontalalignment=\"center\",\n",
        "                     color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n",
        "    plt.show()\n",
        "    return None\n",
        "\n",
        "def gen_fast_plot(x, subject_name, true_time = False):\n",
        "    \"\"\" Takes in a data_dict and a name ex: ['S01'] and plots it \"\"\"\n",
        "    if true_time:\n",
        "        timestamp_vector = (x['timestamp'] - x['timestamp'][0]) / 1000\n",
        "    else:\n",
        "        timestamp_vector = range(0, len(x['timestamp']))\n",
        "        \n",
        "    title_str = ('Plot for subject', subject_name)\n",
        "  \n",
        "    fig = plt.figure(figsize = (25, 10), dpi = 80, facecolor = 'w', edgecolor = 'k')\n",
        "    \n",
        "    plt.subplot(311)\n",
        "  \n",
        "    x_handle = plt.plot(timestamp_vector, x['x_quat'], label ='x')\n",
        "    y_handle = plt.plot(timestamp_vector, x['y_quat'], label ='y')\n",
        "    z_handle = plt.plot(timestamp_vector, x['z_quat'], label ='z')\n",
        "    w_handle = plt.plot(timestamp_vector, x['w_quat'], label ='w')\n",
        "    plt.legend(loc = 'upper right')\n",
        "    plt.title(title_str)\n",
        "   \n",
        "    plt.subplot(312)\n",
        "    \n",
        "    channel_0 = plt.plot(timestamp_vector, x['c0'], label = 'channel_0')\n",
        "    channel_1 = plt.plot(timestamp_vector, x['c1'], label = 'channel_1')\n",
        "    plt.legend(loc = 'upper right')\n",
        "    \n",
        "    plt.subplot(313)\n",
        "    \n",
        "    gesture_labels = plt.plot(timestamp_vector, x['label'], label = 'gesture_labels')\n",
        "    plt.ylim([np.min(x['label']) - 0.5, np.max(x['label']) + 0.5])\n",
        "    plt.legend(loc = 'upper right')\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSL-2Q0hgOBG",
        "colab_type": "text"
      },
      "source": [
        "# Main Execution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "2R4SrdcgWXUw",
        "colab": {}
      },
      "source": [
        "if __name__ == '__main__':   \n",
        "    main(model_choice = MODEL_TO_TRAIN, straight_parameters = None)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}